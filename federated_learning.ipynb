{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this file, we will do a federated learning approach to create a model that can predict the cyber attacks.\n",
    "\n",
    "### Table of Contents :\n",
    "- Hyperparameters approach\n",
    "- Bagging approach\n",
    "- Voting approach\n",
    "- Learned parameters approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier, PassiveAggressiveClassifier, Perceptron, RidgeClassifierCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import BayesSearchCV\n",
    "import joblib\n",
    "import gc\n",
    "from collections import Counter\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import functions_ml as fml\n",
    "\n",
    "# Path to datasets\n",
    "dataset_directory = \".\\Files\\\\\"\n",
    "hyperparameters_directory = \".\\Federated_learning\\\\Hyperparameters_approach\\\\\"\n",
    "voting_directory = \".\\Federated_learning\\\\Voting_approach\\\\\"\n",
    "bagging_directory = \".\\Federated_learning\\\\Bagging_approach\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the datasets in the directory and sort them\n",
    "df_sets = [k for k in os.listdir(dataset_directory) if k.endswith('.csv')]\n",
    "df_sets.sort()\n",
    "\n",
    "# Only use a part of all datasets (10 files)\n",
    "# df_sets = df_sets[:2]\n",
    "\n",
    "# Set 80% of the datasets as models sets and 20% as test sets for federated model\n",
    "df_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "federated_sets = df_sets[int(len(df_sets)*.8):]\n",
    "\n",
    "# Define each column of the dataset and the target column\n",
    "X_columns = [\n",
    "    'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "       'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
    "       'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "       'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "       'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "    'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "       'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
    "       'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "       'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "]\n",
    "y_column = 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build sub-models and search for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 34\n"
     ]
    }
   ],
   "source": [
    "sub_models_datasets, final_models_datasets = fml.get_train_and_test_files()\n",
    "print(len(sub_models_datasets), len(final_models_datasets))\n",
    "\n",
    "X_columns = fml.x_columns(fml.read_csv_file(sub_models_datasets[0]))\n",
    "y_column = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_models_datasets = sub_models_datasets[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [8:15:41<00:00, 220.30s/it]  \n"
     ]
    }
   ],
   "source": [
    "niter = 50\n",
    "ncv = 8\n",
    "\n",
    "# Define performance dataframe\n",
    "df_performance = pd.DataFrame(columns=['file', 'model', 'accuracy', 'precision', 'recall', 'f1', 'params'])\n",
    "\n",
    "for file in tqdm(sub_models_datasets):\n",
    "    # Chargement des données\n",
    "    df = fml.read_csv_file(file, dataset_directory)\n",
    "    X = df.drop(columns=y_column)\n",
    "    y = df[y_column]\n",
    "\n",
    "    # Division en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Création du modèle et recherche par grille\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    search_spaces = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': [None, 10, 100, 1000],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "        'max_leaf_nodes': [None, 10, 100, 1000],\n",
    "        'random_state': [42],\n",
    "    }\n",
    "    search = BayesSearchCV(model, search_spaces, n_iter=niter, cv=ncv, n_jobs=-1, random_state=42)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "\n",
    "    model = DecisionTreeClassifier(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy=accuracy_score(y_test, y_pred)\n",
    "    # class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # Ajout des résultats au dataframe\n",
    "    df_performance = df_performance.append({'file': file, 'model': 'DecisionTreeClassifier', 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'params': best_params}, ignore_index=True)\n",
    "\n",
    "    # Sauvegarde du modèle\n",
    "    joblib.dump(model, f'{hyperparameters_directory}DT_{file[:-4]}.joblib')\n",
    "\n",
    "    # Libération de la mémoire\n",
    "    del model, search, best_params, y_pred, accuracy, precision, recall, f1\n",
    "\n",
    "    # Garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "# Sauvegarde du dataframe\n",
    "df_performance.to_csv(f'{hyperparameters_directory}DT_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build main model with the most common hyperparameters of sub-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 100.0, 'max_features': None, 'max_leaf_nodes': 1000.0, 'min_samples_leaf': 4, 'min_samples_split': 10, 'random_state': 42, 'splitter': 'best'}\n",
      "27 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [02:47<00:00,  6.20s/it]\n",
      "100%|██████████| 7/7 [00:09<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Most common hyperparameters\n",
    "# Load performance dataframe\n",
    "df_performance = pd.read_csv(f'{hyperparameters_directory}DT_performance.csv')\n",
    "\n",
    "def parse_hyperparameters(hyperparam_str):\n",
    "    # Remove 'OrderedDict' and surrounding brackets\n",
    "    clean_str = hyperparam_str.replace('OrderedDict([', '').replace('])', '')\n",
    "\n",
    "    # Initialize an empty dictionary to store hyperparameters\n",
    "    hyperparams = {}\n",
    "\n",
    "    # Regular expression pattern to match key-value pairs\n",
    "    pattern = re.compile(r\"\\('([^']*)', (.*?)\\)\")\n",
    "    matches = pattern.findall(clean_str)\n",
    "\n",
    "    for key, value in matches:\n",
    "        # Strip quotes and leading/trailing whitespace from key and value\n",
    "        key = key.strip()\n",
    "        value = value.strip().strip(\"'\")\n",
    "\n",
    "        # Handle None, numeric, and other string values\n",
    "        if value == 'None':\n",
    "            value = None\n",
    "        elif value.isdigit():\n",
    "            value = int(value)\n",
    "        elif value.replace('.', '', 1).isdigit():\n",
    "            value = float(value)\n",
    "\n",
    "        hyperparams[key] = value\n",
    "\n",
    "    return hyperparams\n",
    "\n",
    "# Apply the parsing function to each row in the 'params' column\n",
    "df_performance['params'] = df_performance['params'].apply(parse_hyperparameters)\n",
    "\n",
    "# Determine the most common hyperparameters\n",
    "common_hyperparams = {}\n",
    "for param in df_performance['params'].iloc[0].keys():\n",
    "    values = df_performance['params'].apply(lambda x: x[param])\n",
    "    most_common = Counter(values).most_common(1)[0][0]\n",
    "    common_hyperparams[param] = most_common\n",
    "\n",
    "print(common_hyperparams)\n",
    "\n",
    "# Ensuring max_depth is an integer\n",
    "if 'max_depth' in common_hyperparams and common_hyperparams['max_depth'] is not None:\n",
    "    common_hyperparams['max_depth'] = int(common_hyperparams['max_depth'])\n",
    "\n",
    "# Ensuring max_features is an integer\n",
    "if 'max_features' in common_hyperparams and common_hyperparams['max_features'] is not None:\n",
    "    common_hyperparams['max_features'] = int(common_hyperparams['max_features'])\n",
    "\n",
    "# Ensuring max_leaf_nodes is an integer\n",
    "if 'max_leaf_nodes' in common_hyperparams and common_hyperparams['max_leaf_nodes'] is not None:\n",
    "    common_hyperparams['max_leaf_nodes'] = int(common_hyperparams['max_leaf_nodes'])\n",
    "\n",
    "# Main performance dataframe\n",
    "model = DecisionTreeClassifier(**common_hyperparams)\n",
    "\n",
    "# If the main performance dataframe already exists, load it\n",
    "if os.path.exists(f'{hyperparameters_directory}DT_main_performance.csv'):\n",
    "    df_main_performance = pd.read_csv(f'{hyperparameters_directory}DT_main_performance.csv')\n",
    "else:\n",
    "    df_main_performance = pd.DataFrame(columns=['model', 'nb_sub_files', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "\n",
    "# Get training and test datasets\n",
    "train_set, test_set = final_models_datasets[:int(len(final_models_datasets)*.8)], final_models_datasets[int(len(final_models_datasets)*.8):]\n",
    "print(len(train_set), len(test_set))\n",
    "\n",
    "for set in tqdm(train_set):\n",
    "    # Load dataset\n",
    "    df = fml.read_csv_file(set, dataset_directory)\n",
    "    X = df.drop(columns=y_column)\n",
    "    y = df[y_column]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X, y)\n",
    "\n",
    "# Test the model\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for set in tqdm(test_set):\n",
    "    # Load dataset\n",
    "    df = fml.read_csv_file(set, dataset_directory)\n",
    "    X = df.drop(columns=y_column)\n",
    "    y = df[y_column]\n",
    "\n",
    "    # Predict labels\n",
    "    y_true.extend(y)\n",
    "    y_pred.extend(model.predict(X))\n",
    "\n",
    "# Compute performance metrics\n",
    "accuracy, precision, recall, f1 = accuracy_score(y_true, y_pred), precision_score(y_true, y_pred, average='macro'), recall_score(y_true, y_pred, average='macro'), f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Save performance and model\n",
    "df_main_performance = df_main_performance.append({'model': model, 'nb_sub_files': len(sub_models_datasets), 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}, ignore_index=True)\n",
    "joblib.dump(model, f'{hyperparameters_directory}DT_main.joblib')\n",
    "df_main_performance.to_csv(f'{hyperparameters_directory}DT_main_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>nb_sub_files</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier(max_depth=1000, max_lea...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.992416</td>\n",
       "      <td>0.856889</td>\n",
       "      <td>0.809058</td>\n",
       "      <td>0.825847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy', ma...</td>\n",
       "      <td>135</td>\n",
       "      <td>0.992448</td>\n",
       "      <td>0.822656</td>\n",
       "      <td>0.796229</td>\n",
       "      <td>0.806956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  nb_sub_files  accuracy  \\\n",
       "0  DecisionTreeClassifier(max_depth=1000, max_lea...            10  0.992416   \n",
       "1  DecisionTreeClassifier(criterion='entropy', ma...           135  0.992448   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0   0.856889  0.809058  0.825847  \n",
       "1   0.822656  0.796229  0.806956  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(f'{hyperparameters_directory}DT_main_performance.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting approach with hyperparameted sub-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 34\n"
     ]
    }
   ],
   "source": [
    "sub_models_datasets, final_models_datasets = fml.get_train_and_test_files()\n",
    "print(len(sub_models_datasets), len(final_models_datasets))\n",
    "\n",
    "X_columns = fml.x_columns(fml.read_csv_file(sub_models_datasets[0]))\n",
    "y_column = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_models_datasets = sub_models_datasets[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 1299.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [4:23:19<00:00, 585.16s/it]\n",
      "100%|██████████| 7/7 [01:57<00:00, 16.72s/it]\n",
      "  4%|▎         | 1/27 [13:39<5:55:12, 819.70s/it]it]\n",
      " 50%|█████     | 1/2 [4:39:43<4:39:43, 16783.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     y \u001b[38;5;241m=\u001b[39m df[y_column]\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[0;32m     37\u001b[0m y_true \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\mlcra\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\ensemble\\_voting.py:351\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    349\u001b[0m transformed_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m--> 351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mlcra\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\ensemble\\_voting.py:83\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVoting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mlcra\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mlcra\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mlcra\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\mlcra\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\mlcra\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mlcra\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\mlcra\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\mlcra\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mlcra\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\ensemble\\_base.py:47\u001b[0m, in \u001b[0;36m_fit_single_estimator\u001b[1;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m---> 47\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[1;32mc:\\Users\\mlcra\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mlcra\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\tree\\_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    449\u001b[0m         splitter,\n\u001b[0;32m    450\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    456\u001b[0m     )\n\u001b[1;32m--> 458\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load all sub models from the directory\n",
    "sub_models = []\n",
    "try:\n",
    "    for file in tqdm(sub_models_datasets):\n",
    "        sub_models.append(joblib.load(f'{hyperparameters_directory}DT_{file[:-4]}.joblib'))\n",
    "except:\n",
    "    raise Exception('Error : no sub models found, please run hyperparameters approach first')\n",
    "\n",
    "# Build the federated model\n",
    "models = []\n",
    "models.append({'model' : VotingClassifier(estimators=[('model_'+str(i), sub_models[i]) for i in range(len(sub_models))], voting='hard'), 'nb_sub_files' : len(sub_models), 'type' : 'hard'})\n",
    "models.append({'model' : VotingClassifier(estimators=[('model_'+str(i), sub_models[i]) for i in range(len(sub_models))], voting='soft'), 'nb_sub_files' : len(sub_models), 'type' : 'soft'})\n",
    "\n",
    "# If the main performance dataframe already exists, load it\n",
    "if os.path.exists(f'{voting_directory}DT_main_performance.csv'):\n",
    "    df_main_performance = pd.read_csv(f'{voting_directory}DT_main_performance.csv')\n",
    "else:\n",
    "    df_main_performance = pd.DataFrame(columns=['model', 'nb_sub_files', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "\n",
    "# Get training and test datasets\n",
    "train_set, test_set = final_models_datasets[:int(len(final_models_datasets)*.8)], final_models_datasets[int(len(final_models_datasets)*.8):]\n",
    "print(len(train_set), len(test_set))\n",
    "\n",
    "# For each model\n",
    "for model in tqdm(models):\n",
    "    # Train the model\n",
    "    for set in tqdm(train_set):\n",
    "        # Load dataset\n",
    "        df = fml.read_csv_file(set, dataset_directory)\n",
    "        X = df.drop(columns=y_column)\n",
    "        y = df[y_column]\n",
    "\n",
    "        # Train the model\n",
    "        model['model'].fit(X, y)\n",
    "\n",
    "    # Test the model\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for set in tqdm(test_set):\n",
    "        # Load dataset\n",
    "        df = fml.read_csv_file(set, dataset_directory)\n",
    "        X = df.drop(columns=y_column)\n",
    "        y = df[y_column]\n",
    "\n",
    "        # Predict labels\n",
    "        y_true.extend(y)\n",
    "        y_pred.extend(model['model'].predict(X))\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy, precision, recall, f1 = accuracy_score(y_true, y_pred), precision_score(y_true, y_pred, average='macro'), recall_score(y_true, y_pred, average='macro'), f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Save performance and model\n",
    "    df_main_performance = df_main_performance.append({'model': model['model'], 'nb_sub_files': model['nb_sub_files'], 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'type' : model['type']}, ignore_index=True)\n",
    "    joblib.dump(model['model'], f'{voting_directory}DT_main_{model[\"type\"]}.joblib')\n",
    "    df_main_performance.to_csv(f'{voting_directory}DT_main_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification models for each files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [08:07<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'Average', 'Accuracy': 0.9919186996812641, 'Precision': 0.992022464355332, 'Recall': 0.9919186996812641, 'F1': 0.991936740398441}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the results\n",
    "results_list = [] \n",
    "\n",
    "# Define models list\n",
    "models = []\n",
    "\n",
    "# Define the scaler method\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# For each dataset\n",
    "for set in tqdm(df_sets):\n",
    "    # Definition of the model to use\n",
    "    ML_model = DecisionTreeClassifier()\n",
    "\n",
    "    # Define the name of the model\n",
    "    ML_neam = \"DecisionTreeClassifier\"\n",
    "\n",
    "    # Load the dataset\n",
    "    d = pd.read_csv(dataset_directory + set)\n",
    "\n",
    "    # Séparez les caractéristiques (X) et les étiquettes (y)\n",
    "    X = d.drop(columns=y_column)\n",
    "    y = d[y_column]\n",
    "\n",
    "    # Divisez les données en ensembles de formation (80%) et de test (20%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit the scaler to the training data\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # Normalize the dataset\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    # print(X_train[:5])\n",
    "    # print(y_train.head())\n",
    "\n",
    "    # Train the model\n",
    "    ML_model.fit(X_train, y_train)\n",
    "\n",
    "    # Delete the dataset from the memory\n",
    "    del d  \n",
    "\n",
    "    # Save the model and add it to the list of models\n",
    "    temp_model = '.\\Models\\FEDERATED\\\\' + set + '.joblib'\n",
    "    joblib.dump(ML_model, temp_model)\n",
    "    models.append(temp_model)\n",
    "\n",
    "\n",
    "    # Predict the labels\n",
    "    preds = ML_model.predict(X_test)\n",
    "\n",
    "    # Results of the model\n",
    "    result = {\n",
    "        'Model': set,\n",
    "        'Accuracy': accuracy_score(y_test, preds),\n",
    "        'Precision': precision_score(y_test, preds, average='weighted'),\n",
    "        'Recall': recall_score(y_test, preds, average='weighted'),\n",
    "        'F1': f1_score(y_test, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "    # Append the result to the temporary list\n",
    "    results_list.append(result)\n",
    "\n",
    "# Create a DataFrame from the list of results\n",
    "results = pd.DataFrame(results_list)\n",
    "\n",
    "# Do the average of all the results\n",
    "final_result = {\n",
    "    'Model': 'Average',\n",
    "    'Accuracy': results['Accuracy'].mean(),\n",
    "    'Precision': results['Precision'].mean(),\n",
    "    'Recall': results['Recall'].mean(),\n",
    "    'F1': results['F1'].mean()\n",
    "}\n",
    "\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sets:  21\n",
      "test_sets:  6\n",
      "Training the final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [01:18<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the final model...\n",
      "Prediction results:\n",
      "y_pred:  1654928\n",
      "y_test:  1654928\n",
      "##### DecisionTreeClassifier() (34 classes) #####\n",
      "accuracy_score:  0.9921404435721675\n",
      "recall_score:  0.8312569709662008\n",
      "precision_score:  0.8174385984988672\n",
      "f1_score:  0.8228106995305607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_final_model(models):\n",
    "  \"\"\"\n",
    "  Crée un modèle final qui prend les paramètres moyens de tous les autres modèles.\n",
    "\n",
    "  Args:\n",
    "    models: Une liste de modèles fédérés.\n",
    "\n",
    "  Returns:\n",
    "    Un nouveau modèle avec les paramètres moyens.\n",
    "  \"\"\"\n",
    "\n",
    "  # Obtenir les paramètres de tous les modèles et faire la moyenne\n",
    "  params = {}\n",
    "  for model in models:\n",
    "    params[model] = model.get_params()\n",
    "\n",
    "  for key in params[models[0]].keys():\n",
    "    params[models[0]][key] = np.mean([params[model][key] for model in models])\n",
    "\n",
    "  # Créer un nouveau modèle avec les paramètres moyens\n",
    "  final_model = models[0].__class__(**params[models[0]])\n",
    "\n",
    "  # Nouveau modèle\n",
    "  return final_model\n",
    "\n",
    "# Charger les modèles fédérés\n",
    "temp_models = []\n",
    "for model_path in models:\n",
    "  model = joblib.load(model_path)\n",
    "  temp_models.append(model)\n",
    "\n",
    "train_sets = federated_sets[:int(len(federated_sets)*.8)]\n",
    "test_sets = federated_sets[int(len(federated_sets)*.8):]\n",
    "print('train_sets: ', len(train_sets))\n",
    "print('test_sets: ', len(test_sets))\n",
    "\n",
    "# Créer le modèle final\n",
    "final_model = create_final_model(temp_models)\n",
    "\n",
    "# Train the final model\n",
    "# Define the scaler method\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# For each training set\n",
    "print(\"Training the final model...\")\n",
    "for train_set in (train_sets):\n",
    "    # Fit the scaler on the training sets\n",
    "    scaler.fit(pd.read_csv(dataset_directory + train_set)[X_columns])\n",
    "\n",
    "# For each dataset of the training set\n",
    "for train_set in tqdm(train_sets):\n",
    "    # Load the dataset\n",
    "    d = pd.read_csv(dataset_directory + train_set)\n",
    "\n",
    "    # Normalize the dataset\n",
    "    d[X_columns] = scaler.transform(d[X_columns])\n",
    "\n",
    "    # Train the model\n",
    "    final_model.fit(d[X_columns], d[y_column])\n",
    "\n",
    "    # Delete the dataset from the memory\n",
    "    del d  \n",
    "\n",
    "# Test the final model\n",
    "# Initialize the list of true labels\n",
    "y_test = []\n",
    "\n",
    "# Initialize the list of predictions\n",
    "y_pred = []\n",
    "\n",
    "# For each dataset of the test set\n",
    "print(\"Testing the final model...\")\n",
    "for test_set in (test_sets):\n",
    "    # Load the dataset\n",
    "    d_test_current = pd.read_csv(dataset_directory + test_set)\n",
    "\n",
    "    # Normalize the dataset\n",
    "    d_test_current[X_columns] = scaler.transform(d_test_current[X_columns])\n",
    "\n",
    "    # Add the true labels to the list\n",
    "    y_test += list(d_test_current[y_column].values)\n",
    "\n",
    "    # Predict the labels\n",
    "    y_pred += list(final_model.predict(d_test_current[X_columns]))\n",
    "\n",
    "    # Delete the dataset from the memory\n",
    "    del d_test_current\n",
    "\n",
    "# For each prediction\n",
    "print(\"Prediction results:\")\n",
    "\n",
    "print('y_pred: ', len(y_pred))\n",
    "print('y_test: ', len(y_test))\n",
    "print(f\"##### {final_model} (34 classes) #####\")\n",
    "print('accuracy_score: ', accuracy_score(y_pred, y_test))\n",
    "print('recall_score: ', recall_score(y_pred, y_test, average='macro'))\n",
    "print('precision_score: ', precision_score(y_pred, y_test, average='macro'))\n",
    "print('f1_score: ', f1_score(y_pred, y_test, average='macro'))\n",
    "print()\n",
    "\n",
    "\n",
    "# # Sauvegarder le modèle central fédéré\n",
    "# joblib.dump(central_federated_model, '.\\\\Models\\\\FEDERATED\\\\central_federated_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
