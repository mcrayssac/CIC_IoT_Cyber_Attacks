{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this file, we will do a federated learning approach to create a model that can predict the cyber attacks.\n",
    "\n",
    "### Table of Contents :\n",
    "- Hyperparameters approach\n",
    "- Bagging approach\n",
    "- Voting approach\n",
    "- Learned parameters approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier, PassiveAggressiveClassifier, Perceptron, RidgeClassifierCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import BayesSearchCV\n",
    "import joblib\n",
    "import gc\n",
    "from collections import Counter\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import functions_ml as fml\n",
    "\n",
    "# Path to datasets\n",
    "dataset_directory = \".\\Files\\\\\"\n",
    "hyperparameters_directory = \".\\Federated_learning\\\\Hyperparameters_approach\\\\\"\n",
    "voting_directory = \".\\Federated_learning\\\\Voting_approach\\\\\"\n",
    "stacking_directory = \".\\Federated_learning\\\\Stacking_approach\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the datasets in the directory and sort them\n",
    "df_sets = [k for k in os.listdir(dataset_directory) if k.endswith('.csv')]\n",
    "df_sets.sort()\n",
    "\n",
    "# Only use a part of all datasets (10 files)\n",
    "# df_sets = df_sets[:2]\n",
    "\n",
    "# Set 80% of the datasets as models sets and 20% as test sets for federated model\n",
    "df_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "federated_sets = df_sets[int(len(df_sets)*.8):]\n",
    "\n",
    "# Define each column of the dataset and the target column\n",
    "X_columns = [\n",
    "    'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "       'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
    "       'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "       'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "       'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "    'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "       'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
    "       'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "       'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "]\n",
    "y_column = 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build sub-models and search for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 34\n"
     ]
    }
   ],
   "source": [
    "sub_models_datasets, final_models_datasets = fml.get_train_and_test_files()\n",
    "print(len(sub_models_datasets), len(final_models_datasets))\n",
    "\n",
    "X_columns = fml.x_columns(fml.read_csv_file(sub_models_datasets[0]))\n",
    "y_column = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_models_datasets = sub_models_datasets[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [8:15:41<00:00, 220.30s/it]  \n"
     ]
    }
   ],
   "source": [
    "niter = 50\n",
    "ncv = 8\n",
    "\n",
    "# Define performance dataframe\n",
    "df_performance = pd.DataFrame(columns=['file', 'model', 'accuracy', 'precision', 'recall', 'f1', 'params'])\n",
    "\n",
    "for file in tqdm(sub_models_datasets):\n",
    "    # Chargement des données\n",
    "    df = fml.read_csv_file(file, dataset_directory)\n",
    "    X = df.drop(columns=y_column)\n",
    "    y = df[y_column]\n",
    "\n",
    "    # Division en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Création du modèle et recherche par grille\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    search_spaces = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': [None, 10, 100, 1000],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "        'max_leaf_nodes': [None, 10, 100, 1000],\n",
    "        'random_state': [42],\n",
    "    }\n",
    "    search = BayesSearchCV(model, search_spaces, n_iter=niter, cv=ncv, n_jobs=-1, random_state=42)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "\n",
    "    model = DecisionTreeClassifier(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy=accuracy_score(y_test, y_pred)\n",
    "    # class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # Ajout des résultats au dataframe\n",
    "    df_performance = df_performance.append({'file': file, 'model': 'DecisionTreeClassifier', 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'params': best_params}, ignore_index=True)\n",
    "\n",
    "    # Sauvegarde du modèle\n",
    "    joblib.dump(model, f'{hyperparameters_directory}DT_{file[:-4]}.joblib')\n",
    "\n",
    "    # Libération de la mémoire\n",
    "    del model, search, best_params, y_pred, accuracy, precision, recall, f1\n",
    "\n",
    "    # Garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "# Sauvegarde du dataframe\n",
    "df_performance.to_csv(f'{hyperparameters_directory}DT_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build main model with the most common hyperparameters of sub-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 100.0, 'max_features': None, 'max_leaf_nodes': 1000.0, 'min_samples_leaf': 4, 'min_samples_split': 10, 'random_state': 42, 'splitter': 'best'}\n",
      "27 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [02:47<00:00,  6.20s/it]\n",
      "100%|██████████| 7/7 [00:09<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Most common hyperparameters\n",
    "# Load performance dataframe\n",
    "df_performance = pd.read_csv(f'{hyperparameters_directory}DT_performance.csv')\n",
    "\n",
    "def parse_hyperparameters(hyperparam_str):\n",
    "    # Remove 'OrderedDict' and surrounding brackets\n",
    "    clean_str = hyperparam_str.replace('OrderedDict([', '').replace('])', '')\n",
    "\n",
    "    # Initialize an empty dictionary to store hyperparameters\n",
    "    hyperparams = {}\n",
    "\n",
    "    # Regular expression pattern to match key-value pairs\n",
    "    pattern = re.compile(r\"\\('([^']*)', (.*?)\\)\")\n",
    "    matches = pattern.findall(clean_str)\n",
    "\n",
    "    for key, value in matches:\n",
    "        # Strip quotes and leading/trailing whitespace from key and value\n",
    "        key = key.strip()\n",
    "        value = value.strip().strip(\"'\")\n",
    "\n",
    "        # Handle None, numeric, and other string values\n",
    "        if value == 'None':\n",
    "            value = None\n",
    "        elif value.isdigit():\n",
    "            value = int(value)\n",
    "        elif value.replace('.', '', 1).isdigit():\n",
    "            value = float(value)\n",
    "\n",
    "        hyperparams[key] = value\n",
    "\n",
    "    return hyperparams\n",
    "\n",
    "# Apply the parsing function to each row in the 'params' column\n",
    "df_performance['params'] = df_performance['params'].apply(parse_hyperparameters)\n",
    "\n",
    "# Determine the most common hyperparameters\n",
    "common_hyperparams = {}\n",
    "for param in df_performance['params'].iloc[0].keys():\n",
    "    values = df_performance['params'].apply(lambda x: x[param])\n",
    "    most_common = Counter(values).most_common(1)[0][0]\n",
    "    common_hyperparams[param] = most_common\n",
    "\n",
    "print(common_hyperparams)\n",
    "\n",
    "# Ensuring max_depth is an integer\n",
    "if 'max_depth' in common_hyperparams and common_hyperparams['max_depth'] is not None:\n",
    "    common_hyperparams['max_depth'] = int(common_hyperparams['max_depth'])\n",
    "\n",
    "# Ensuring max_features is an integer\n",
    "if 'max_features' in common_hyperparams and common_hyperparams['max_features'] is not None:\n",
    "    common_hyperparams['max_features'] = int(common_hyperparams['max_features'])\n",
    "\n",
    "# Ensuring max_leaf_nodes is an integer\n",
    "if 'max_leaf_nodes' in common_hyperparams and common_hyperparams['max_leaf_nodes'] is not None:\n",
    "    common_hyperparams['max_leaf_nodes'] = int(common_hyperparams['max_leaf_nodes'])\n",
    "\n",
    "# Main performance dataframe\n",
    "model = DecisionTreeClassifier(**common_hyperparams)\n",
    "\n",
    "# If the main performance dataframe already exists, load it\n",
    "if os.path.exists(f'{hyperparameters_directory}DT_main_performance.csv'):\n",
    "    df_main_performance = pd.read_csv(f'{hyperparameters_directory}DT_main_performance.csv')\n",
    "else:\n",
    "    df_main_performance = pd.DataFrame(columns=['model', 'nb_sub_files', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "\n",
    "# Get training and test datasets\n",
    "train_set, test_set = final_models_datasets[:int(len(final_models_datasets)*.8)], final_models_datasets[int(len(final_models_datasets)*.8):]\n",
    "print(len(train_set), len(test_set))\n",
    "\n",
    "for set in tqdm(train_set):\n",
    "    # Load dataset\n",
    "    df = fml.read_csv_file(set, dataset_directory)\n",
    "    X = df.drop(columns=y_column)\n",
    "    y = df[y_column]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X, y)\n",
    "\n",
    "# Test the model\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for set in tqdm(test_set):\n",
    "    # Load dataset\n",
    "    df = fml.read_csv_file(set, dataset_directory)\n",
    "    X = df.drop(columns=y_column)\n",
    "    y = df[y_column]\n",
    "\n",
    "    # Predict labels\n",
    "    y_true.extend(y)\n",
    "    y_pred.extend(model.predict(X))\n",
    "\n",
    "# Compute performance metrics\n",
    "accuracy, precision, recall, f1 = accuracy_score(y_true, y_pred), precision_score(y_true, y_pred, average='macro'), recall_score(y_true, y_pred, average='macro'), f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Save performance and model\n",
    "df_main_performance = df_main_performance.append({'model': model, 'nb_sub_files': len(sub_models_datasets), 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}, ignore_index=True)\n",
    "joblib.dump(model, f'{hyperparameters_directory}DT_main.joblib')\n",
    "df_main_performance.to_csv(f'{hyperparameters_directory}DT_main_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>nb_sub_files</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier(max_depth=1000, max_lea...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.992416</td>\n",
       "      <td>0.856889</td>\n",
       "      <td>0.809058</td>\n",
       "      <td>0.825847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy', ma...</td>\n",
       "      <td>135</td>\n",
       "      <td>0.992448</td>\n",
       "      <td>0.822656</td>\n",
       "      <td>0.796229</td>\n",
       "      <td>0.806956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  nb_sub_files  accuracy  \\\n",
       "0  DecisionTreeClassifier(max_depth=1000, max_lea...            10  0.992416   \n",
       "1  DecisionTreeClassifier(criterion='entropy', ma...           135  0.992448   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0   0.856889  0.809058  0.825847  \n",
       "1   0.822656  0.796229  0.806956  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(f'{hyperparameters_directory}DT_main_performance.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting approach with hyperparameted sub-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 34\n"
     ]
    }
   ],
   "source": [
    "sub_models_datasets, final_models_datasets = fml.get_train_and_test_files()\n",
    "print(len(sub_models_datasets), len(final_models_datasets))\n",
    "\n",
    "X_columns = fml.x_columns(fml.read_csv_file(sub_models_datasets[0]))\n",
    "y_column = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_models_datasets = sub_models_datasets[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sub models from the directory\n",
    "sub_models = []\n",
    "try:\n",
    "    for file in tqdm(sub_models_datasets):\n",
    "        sub_models.append(joblib.load(f'{hyperparameters_directory}DT_{file[:-4]}.joblib'))\n",
    "except:\n",
    "    raise Exception('Error : no sub models found, please run hyperparameters approach first')\n",
    "\n",
    "# Build the federated model\n",
    "models = []\n",
    "models.append({'model' : VotingClassifier(estimators=[('model_'+str(i), sub_models[i]) for i in range(len(sub_models))], voting='hard'), 'nb_sub_files' : len(sub_models), 'type' : 'hard'})\n",
    "models.append({'model' : VotingClassifier(estimators=[('model_'+str(i), sub_models[i]) for i in range(len(sub_models))], voting='soft'), 'nb_sub_files' : len(sub_models), 'type' : 'soft'})\n",
    "\n",
    "# If the main performance dataframe already exists, load it\n",
    "if os.path.exists(f'{voting_directory}DT_main_performance.csv'):\n",
    "    df_main_performance = pd.read_csv(f'{voting_directory}DT_main_performance.csv')\n",
    "else:\n",
    "    df_main_performance = pd.DataFrame(columns=['model', 'nb_sub_files', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "\n",
    "# Get training and test datasets\n",
    "train_set, test_set = final_models_datasets[:int(len(final_models_datasets)*.8)], final_models_datasets[int(len(final_models_datasets)*.8):]\n",
    "print(len(train_set), len(test_set))\n",
    "\n",
    "# For each model\n",
    "for model in tqdm(models):\n",
    "    # Train the model\n",
    "    for set in tqdm(train_set):\n",
    "        # Load dataset\n",
    "        df = fml.read_csv_file(set, dataset_directory)\n",
    "        X = df.drop(columns=y_column)\n",
    "        y = df[y_column]\n",
    "\n",
    "        # Train the model\n",
    "        model['model'].fit(X, y)\n",
    "\n",
    "    # Test the model\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for set in tqdm(test_set):\n",
    "        # Load dataset\n",
    "        df = fml.read_csv_file(set, dataset_directory)\n",
    "        X = df.drop(columns=y_column)\n",
    "        y = df[y_column]\n",
    "\n",
    "        # Predict labels\n",
    "        y_true.extend(y)\n",
    "        y_pred.extend(model['model'].predict(X))\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy, precision, recall, f1 = accuracy_score(y_true, y_pred), precision_score(y_true, y_pred, average='macro'), recall_score(y_true, y_pred, average='macro'), f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Save performance and model\n",
    "    df_main_performance = df_main_performance.append({'model': model['model'], 'nb_sub_files': model['nb_sub_files'], 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'type' : model['type']}, ignore_index=True)\n",
    "    joblib.dump(model['model'], f'{voting_directory}DT_main_{model[\"type\"]}.joblib')\n",
    "    df_main_performance.to_csv(f'{voting_directory}DT_main_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking approach (final estimator be which one who has the best score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking approach with hyperparameted sub-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_models_datasets, final_models_datasets = fml.get_train_and_test_files()\n",
    "print(len(sub_models_datasets), len(final_models_datasets))\n",
    "\n",
    "X_columns = fml.x_columns(fml.read_csv_file(sub_models_datasets[0]))\n",
    "y_column = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_models_datasets = sub_models_datasets[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 869.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=100, min_samples_leaf=4, min_samples_split=5,\n",
      "                       random_state=42) 0.9925245358609652 0.9030795932281324 0.8538368414214664 0.8726239087971481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load performance dataframe\n",
    "df_performance = pd.read_csv(f'{hyperparameters_directory}DT_performance.csv')\n",
    "\n",
    "# Dictionary to store models and their metrics\n",
    "models_with_metrics = {}\n",
    "\n",
    "# Load all sub models and associate them with their performance metrics\n",
    "try:\n",
    "    for file in tqdm(sub_models_datasets):\n",
    "        model_name = f'DT_{file[:-4]}'\n",
    "        model = joblib.load(f'{hyperparameters_directory}{model_name}.joblib')\n",
    "        # Find the row in df_performance that corresponds to this model\n",
    "        model_metrics = df_performance[df_performance['file'] == file].iloc[0]\n",
    "        models_with_metrics[model_name] = {\n",
    "            'model': model,\n",
    "            'accuracy': model_metrics['accuracy'],\n",
    "            'precision': model_metrics['precision'],\n",
    "            'recall': model_metrics['recall'],\n",
    "            'f1': model_metrics['f1']\n",
    "        }\n",
    "except Exception as e:\n",
    "    raise Exception(f'Error loading models: {e}')\n",
    "\n",
    "# Calculate a combined score for each model and select the best one\n",
    "for model_name, metrics in models_with_metrics.items():\n",
    "    combined_score = (metrics['accuracy'] + metrics['precision'] + metrics['recall'] + metrics['f1']) / 4\n",
    "    models_with_metrics[model_name]['combined_score'] = combined_score\n",
    "\n",
    "# Sort models based on the combined score\n",
    "best_models_sorted = sorted(models_with_metrics.values(), key=lambda x: x['combined_score'], reverse=True)\n",
    "\n",
    "# Select the best model (final estimator for stacking) and other models to use for bagging\n",
    "best_model = best_models_sorted[0]\n",
    "other_models = best_models_sorted[1:]\n",
    "\n",
    "# Print the best model\n",
    "print(best_model['model'], best_model['accuracy'], best_model['precision'], best_model['recall'], best_model['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Build the federated model\n",
    "models = []\n",
    "models.append({'model' : StackingClassifier(estimators=[('model_'+str(i), other_models[i]['model']) for i in range(len(other_models))], final_estimator=best_model['model']), 'nb_sub_files' : len(other_models) + 1, 'type' : 'stacking'})\n",
    "\n",
    "# If the main performance dataframe already exists, load it\n",
    "if os.path.exists(f'{stacking_directory}DT_main_performance.csv'):\n",
    "    df_main_performance = pd.read_csv(f'{stacking_directory}DT_main_performance.csv')\n",
    "else:\n",
    "    df_main_performance = pd.DataFrame(columns=['model', 'nb_sub_files', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "\n",
    "# Get training and test datasets\n",
    "train_set, test_set = final_models_datasets[:int(len(final_models_datasets)*.8)], final_models_datasets[int(len(final_models_datasets)*.8):]\n",
    "print(len(train_set), len(test_set))\n",
    "\n",
    "# For each model\n",
    "for model in tqdm(models):\n",
    "    # Train the model\n",
    "    for set in tqdm(train_set):\n",
    "        # Load dataset\n",
    "        df = fml.read_csv_file(set, dataset_directory)\n",
    "        X = df.drop(columns=y_column)\n",
    "        y = df[y_column]\n",
    "\n",
    "        # Train the model\n",
    "        model['model'].fit(X, y)\n",
    "\n",
    "    # Test the model\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for set in tqdm(test_set):\n",
    "        # Load dataset\n",
    "        df = fml.read_csv_file(set, dataset_directory)\n",
    "        X = df.drop(columns=y_column)\n",
    "        y = df[y_column]\n",
    "\n",
    "        # Predict labels\n",
    "        y_true.extend(y)\n",
    "        y_pred.extend(model['model'].predict(X))\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy, precision, recall, f1 = accuracy_score(y_true, y_pred), precision_score(y_true, y_pred, average='macro'), recall_score(y_true, y_pred, average='macro'), f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Save performance and model\n",
    "    df_main_performance = df_main_performance.append({'model': model['model'], 'nb_sub_files': model['nb_sub_files'], 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'type' : model['type']}, ignore_index=True)\n",
    "    joblib.dump(model['model'], f'{stacking_directory}DT_main_{model[\"type\"]}.joblib')\n",
    "    df_main_performance.to_csv(f'{stacking_directory}DT_main_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification models for each files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [08:07<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'Average', 'Accuracy': 0.9919186996812641, 'Precision': 0.992022464355332, 'Recall': 0.9919186996812641, 'F1': 0.991936740398441}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the results\n",
    "results_list = [] \n",
    "\n",
    "# Define models list\n",
    "models = []\n",
    "\n",
    "# Define the scaler method\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# For each dataset\n",
    "for set in tqdm(df_sets):\n",
    "    # Definition of the model to use\n",
    "    ML_model = DecisionTreeClassifier()\n",
    "\n",
    "    # Define the name of the model\n",
    "    ML_neam = \"DecisionTreeClassifier\"\n",
    "\n",
    "    # Load the dataset\n",
    "    d = pd.read_csv(dataset_directory + set)\n",
    "\n",
    "    # Séparez les caractéristiques (X) et les étiquettes (y)\n",
    "    X = d.drop(columns=y_column)\n",
    "    y = d[y_column]\n",
    "\n",
    "    # Divisez les données en ensembles de formation (80%) et de test (20%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit the scaler to the training data\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # Normalize the dataset\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    # print(X_train[:5])\n",
    "    # print(y_train.head())\n",
    "\n",
    "    # Train the model\n",
    "    ML_model.fit(X_train, y_train)\n",
    "\n",
    "    # Delete the dataset from the memory\n",
    "    del d  \n",
    "\n",
    "    # Save the model and add it to the list of models\n",
    "    temp_model = '.\\Models\\FEDERATED\\\\' + set + '.joblib'\n",
    "    joblib.dump(ML_model, temp_model)\n",
    "    models.append(temp_model)\n",
    "\n",
    "\n",
    "    # Predict the labels\n",
    "    preds = ML_model.predict(X_test)\n",
    "\n",
    "    # Results of the model\n",
    "    result = {\n",
    "        'Model': set,\n",
    "        'Accuracy': accuracy_score(y_test, preds),\n",
    "        'Precision': precision_score(y_test, preds, average='weighted'),\n",
    "        'Recall': recall_score(y_test, preds, average='weighted'),\n",
    "        'F1': f1_score(y_test, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "    # Append the result to the temporary list\n",
    "    results_list.append(result)\n",
    "\n",
    "# Create a DataFrame from the list of results\n",
    "results = pd.DataFrame(results_list)\n",
    "\n",
    "# Do the average of all the results\n",
    "final_result = {\n",
    "    'Model': 'Average',\n",
    "    'Accuracy': results['Accuracy'].mean(),\n",
    "    'Precision': results['Precision'].mean(),\n",
    "    'Recall': results['Recall'].mean(),\n",
    "    'F1': results['F1'].mean()\n",
    "}\n",
    "\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sets:  21\n",
      "test_sets:  6\n",
      "Training the final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [01:18<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the final model...\n",
      "Prediction results:\n",
      "y_pred:  1654928\n",
      "y_test:  1654928\n",
      "##### DecisionTreeClassifier() (34 classes) #####\n",
      "accuracy_score:  0.9921404435721675\n",
      "recall_score:  0.8312569709662008\n",
      "precision_score:  0.8174385984988672\n",
      "f1_score:  0.8228106995305607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_final_model(models):\n",
    "  \"\"\"\n",
    "  Crée un modèle final qui prend les paramètres moyens de tous les autres modèles.\n",
    "\n",
    "  Args:\n",
    "    models: Une liste de modèles fédérés.\n",
    "\n",
    "  Returns:\n",
    "    Un nouveau modèle avec les paramètres moyens.\n",
    "  \"\"\"\n",
    "\n",
    "  # Obtenir les paramètres de tous les modèles et faire la moyenne\n",
    "  params = {}\n",
    "  for model in models:\n",
    "    params[model] = model.get_params()\n",
    "\n",
    "  for key in params[models[0]].keys():\n",
    "    params[models[0]][key] = np.mean([params[model][key] for model in models])\n",
    "\n",
    "  # Créer un nouveau modèle avec les paramètres moyens\n",
    "  final_model = models[0].__class__(**params[models[0]])\n",
    "\n",
    "  # Nouveau modèle\n",
    "  return final_model\n",
    "\n",
    "# Charger les modèles fédérés\n",
    "temp_models = []\n",
    "for model_path in models:\n",
    "  model = joblib.load(model_path)\n",
    "  temp_models.append(model)\n",
    "\n",
    "train_sets = federated_sets[:int(len(federated_sets)*.8)]\n",
    "test_sets = federated_sets[int(len(federated_sets)*.8):]\n",
    "print('train_sets: ', len(train_sets))\n",
    "print('test_sets: ', len(test_sets))\n",
    "\n",
    "# Créer le modèle final\n",
    "final_model = create_final_model(temp_models)\n",
    "\n",
    "# Train the final model\n",
    "# Define the scaler method\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# For each training set\n",
    "print(\"Training the final model...\")\n",
    "for train_set in (train_sets):\n",
    "    # Fit the scaler on the training sets\n",
    "    scaler.fit(pd.read_csv(dataset_directory + train_set)[X_columns])\n",
    "\n",
    "# For each dataset of the training set\n",
    "for train_set in tqdm(train_sets):\n",
    "    # Load the dataset\n",
    "    d = pd.read_csv(dataset_directory + train_set)\n",
    "\n",
    "    # Normalize the dataset\n",
    "    d[X_columns] = scaler.transform(d[X_columns])\n",
    "\n",
    "    # Train the model\n",
    "    final_model.fit(d[X_columns], d[y_column])\n",
    "\n",
    "    # Delete the dataset from the memory\n",
    "    del d  \n",
    "\n",
    "# Test the final model\n",
    "# Initialize the list of true labels\n",
    "y_test = []\n",
    "\n",
    "# Initialize the list of predictions\n",
    "y_pred = []\n",
    "\n",
    "# For each dataset of the test set\n",
    "print(\"Testing the final model...\")\n",
    "for test_set in (test_sets):\n",
    "    # Load the dataset\n",
    "    d_test_current = pd.read_csv(dataset_directory + test_set)\n",
    "\n",
    "    # Normalize the dataset\n",
    "    d_test_current[X_columns] = scaler.transform(d_test_current[X_columns])\n",
    "\n",
    "    # Add the true labels to the list\n",
    "    y_test += list(d_test_current[y_column].values)\n",
    "\n",
    "    # Predict the labels\n",
    "    y_pred += list(final_model.predict(d_test_current[X_columns]))\n",
    "\n",
    "    # Delete the dataset from the memory\n",
    "    del d_test_current\n",
    "\n",
    "# For each prediction\n",
    "print(\"Prediction results:\")\n",
    "\n",
    "print('y_pred: ', len(y_pred))\n",
    "print('y_test: ', len(y_test))\n",
    "print(f\"##### {final_model} (34 classes) #####\")\n",
    "print('accuracy_score: ', accuracy_score(y_pred, y_test))\n",
    "print('recall_score: ', recall_score(y_pred, y_test, average='macro'))\n",
    "print('precision_score: ', precision_score(y_pred, y_test, average='macro'))\n",
    "print('f1_score: ', f1_score(y_pred, y_test, average='macro'))\n",
    "print()\n",
    "\n",
    "\n",
    "# # Sauvegarder le modèle central fédéré\n",
    "# joblib.dump(central_federated_model, '.\\\\Models\\\\FEDERATED\\\\central_federated_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
