{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this file, we will import several models of specific cyber attack classifier (based on DecisionTreeClassifier, best classfier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier, PassiveAggressiveClassifier, Perceptron, RidgeClassifierCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import gc\n",
    "\n",
    "# Path to datasets\n",
    "DATASET_DIRECTORY = \".\\Files\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the datasets in the directory and sort them\n",
    "df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "df_sets.sort()\n",
    "\n",
    "# Only use a part of all datasets (10 files)\n",
    "# df_sets = df_sets[:2]\n",
    "\n",
    "# Set 80% of the datasets as models sets and 20% as test sets for federated model\n",
    "df_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "federated_sets = df_sets[int(len(df_sets)*.8):]\n",
    "\n",
    "# Define each column of the dataset and the target column\n",
    "X_columns = [\n",
    "    'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "       'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
    "       'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "       'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "       'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "    'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "       'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
    "       'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "       'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "]\n",
    "y_column = 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification models for each files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [08:07<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'Average', 'Accuracy': 0.9919186996812641, 'Precision': 0.992022464355332, 'Recall': 0.9919186996812641, 'F1': 0.991936740398441}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the results\n",
    "results_list = [] \n",
    "\n",
    "# Define models list\n",
    "models = []\n",
    "\n",
    "# Define the scaler method\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# For each dataset\n",
    "for set in tqdm(df_sets):\n",
    "    # Definition of the model to use\n",
    "    ML_model = DecisionTreeClassifier()\n",
    "\n",
    "    # Define the name of the model\n",
    "    ML_neam = \"DecisionTreeClassifier\"\n",
    "\n",
    "    # Load the dataset\n",
    "    d = pd.read_csv(DATASET_DIRECTORY + set)\n",
    "\n",
    "    # Séparez les caractéristiques (X) et les étiquettes (y)\n",
    "    X = d.drop(columns=y_column)\n",
    "    y = d[y_column]\n",
    "\n",
    "    # Divisez les données en ensembles de formation (80%) et de test (20%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit the scaler to the training data\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # Normalize the dataset\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    # print(X_train[:5])\n",
    "    # print(y_train.head())\n",
    "\n",
    "    # Train the model\n",
    "    ML_model.fit(X_train, y_train)\n",
    "\n",
    "    # Delete the dataset from the memory\n",
    "    del d  \n",
    "\n",
    "    # Save the model and add it to the list of models\n",
    "    temp_model = '.\\Models\\FEDERATED\\\\' + set + '.joblib'\n",
    "    joblib.dump(ML_model, temp_model)\n",
    "    models.append(temp_model)\n",
    "\n",
    "\n",
    "    # Predict the labels\n",
    "    preds = ML_model.predict(X_test)\n",
    "\n",
    "    # Results of the model\n",
    "    result = {\n",
    "        'Model': set,\n",
    "        'Accuracy': accuracy_score(y_test, preds),\n",
    "        'Precision': precision_score(y_test, preds, average='weighted'),\n",
    "        'Recall': recall_score(y_test, preds, average='weighted'),\n",
    "        'F1': f1_score(y_test, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "    # Append the result to the temporary list\n",
    "    results_list.append(result)\n",
    "\n",
    "# Create a DataFrame from the list of results\n",
    "results = pd.DataFrame(results_list)\n",
    "\n",
    "# Do the average of all the results\n",
    "final_result = {\n",
    "    'Model': 'Average',\n",
    "    'Accuracy': results['Accuracy'].mean(),\n",
    "    'Precision': results['Precision'].mean(),\n",
    "    'Recall': results['Recall'].mean(),\n",
    "    'F1': results['F1'].mean()\n",
    "}\n",
    "\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Federated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sets:  21\n",
      "test_sets:  6\n",
      "Training the final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [01:18<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the final model...\n",
      "Prediction results:\n",
      "y_pred:  1654928\n",
      "y_test:  1654928\n",
      "##### DecisionTreeClassifier() (34 classes) #####\n",
      "accuracy_score:  0.9921404435721675\n",
      "recall_score:  0.8312569709662008\n",
      "precision_score:  0.8174385984988672\n",
      "f1_score:  0.8228106995305607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_final_model(models):\n",
    "  \"\"\"\n",
    "  Crée un modèle final qui prend les paramètres moyens de tous les autres modèles.\n",
    "\n",
    "  Args:\n",
    "    models: Une liste de modèles fédérés.\n",
    "\n",
    "  Returns:\n",
    "    Un nouveau modèle avec les paramètres moyens.\n",
    "  \"\"\"\n",
    "\n",
    "  # Obtenir les paramètres de tous les modèles\n",
    "  params = {}\n",
    "  for model in models:\n",
    "    params.update(model.get_params())\n",
    "\n",
    "  # Créer un nouveau modèle avec les paramètres moyens\n",
    "  final_model = DecisionTreeClassifier()\n",
    "  final_model.set_params(**params)\n",
    "\n",
    "  # Nouveau modèle\n",
    "  return final_model\n",
    "\n",
    "# Charger les modèles fédérés\n",
    "temp_models = []\n",
    "for model_path in models:\n",
    "  model = joblib.load(model_path)\n",
    "  temp_models.append(model)\n",
    "\n",
    "train_sets = federated_sets[:int(len(federated_sets)*.8)]\n",
    "test_sets = federated_sets[int(len(federated_sets)*.8):]\n",
    "print('train_sets: ', len(train_sets))\n",
    "print('test_sets: ', len(test_sets))\n",
    "\n",
    "# Créer le modèle final\n",
    "final_model = create_final_model(temp_models)\n",
    "\n",
    "# Train the final model\n",
    "# Define the scaler method\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# For each training set\n",
    "print(\"Training the final model...\")\n",
    "for train_set in (train_sets):\n",
    "    # Fit the scaler on the training sets\n",
    "    scaler.fit(pd.read_csv(DATASET_DIRECTORY + train_set)[X_columns])\n",
    "\n",
    "# For each dataset of the training set\n",
    "for train_set in tqdm(train_sets):\n",
    "    # Load the dataset\n",
    "    d = pd.read_csv(DATASET_DIRECTORY + train_set)\n",
    "\n",
    "    # Normalize the dataset\n",
    "    d[X_columns] = scaler.transform(d[X_columns])\n",
    "\n",
    "    # Train the model\n",
    "    final_model.fit(d[X_columns], d[y_column])\n",
    "\n",
    "    # Delete the dataset from the memory\n",
    "    del d  \n",
    "\n",
    "# Test the final model\n",
    "# Initialize the list of true labels\n",
    "y_test = []\n",
    "\n",
    "# Initialize the list of predictions\n",
    "y_pred = []\n",
    "\n",
    "# For each dataset of the test set\n",
    "print(\"Testing the final model...\")\n",
    "for test_set in (test_sets):\n",
    "    # Load the dataset\n",
    "    d_test_current = pd.read_csv(DATASET_DIRECTORY + test_set)\n",
    "\n",
    "    # Normalize the dataset\n",
    "    d_test_current[X_columns] = scaler.transform(d_test_current[X_columns])\n",
    "\n",
    "    # Add the true labels to the list\n",
    "    y_test += list(d_test_current[y_column].values)\n",
    "\n",
    "    # Predict the labels\n",
    "    y_pred += list(final_model.predict(d_test_current[X_columns]))\n",
    "\n",
    "    # Delete the dataset from the memory\n",
    "    del d_test_current\n",
    "\n",
    "# For each prediction\n",
    "print(\"Prediction results:\")\n",
    "\n",
    "print('y_pred: ', len(y_pred))\n",
    "print('y_test: ', len(y_test))\n",
    "print(f\"##### {final_model} (34 classes) #####\")\n",
    "print('accuracy_score: ', accuracy_score(y_pred, y_test))\n",
    "print('recall_score: ', recall_score(y_pred, y_test, average='macro'))\n",
    "print('precision_score: ', precision_score(y_pred, y_test, average='macro'))\n",
    "print('f1_score: ', f1_score(y_pred, y_test, average='macro'))\n",
    "print()\n",
    "\n",
    "\n",
    "# # Sauvegarder le modèle central fédéré\n",
    "# joblib.dump(central_federated_model, '.\\\\Models\\\\FEDERATED\\\\central_federated_model.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
